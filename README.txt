A.Swati Barariasb4700Jaidev Shah<UNI>B. List of files in the tar.gzSearch.pySparseVectorUpdates.pypreprocess.pyword_order.pyC. Commands necessary to install the required software and dependencies and then to run the program:TODOD.Description:We start by taking some necessary user inputs i.e Api Key, Engine Key, query, and precision. First, we preprocess and tokenize the query and obtain the relevant search string. Using this and the  above parameters we fetch the top 10 google search results. Feedback is taken from the user for each of the 10 results returned. Hence providing us with a binary label for relevance.The title, url, snippet, relevance feedback of each result is stored in a dictionary. Non-HTML files are filtered out and not stored in the dictionary. Precision is calculated by observing how many out of the valid results were relevant.If precision is below the desired precision, then the query is expandedThe snippet along with title is preprocessed to obtain a cleaner string which will be later used. These strings along with their relevance are used to compute the top candidates for query expansion keywords. The candidate keywords are also permuted to obtain the most relevant order in which they should be used. Query is rerun for search results; feedback is taken and query expansion done until desired precision is reached.E.Query Expansion:We use preprocessing to clean the title and snippet, strip it, stem it, remove stopwords, remove any special characters and other non-relevant items. We also do pos tagging of the words and finally append the processed snippet and pos tags dictionary to docdict. The nltk package is heavily used for these steps. Using the docdict we create two sets, one for the ones corresponding to relevant documents and the other corresponding to non-relevant documents. We use these to create dictionaries which further enable us to store term frequency for a term and corresponding doc_id. We also computer and store the document frequency mappings into a dictionary. For every word, using the above information, we obtain the tf and idf score. We take the product of these, and then use the POS tags which we had initially given, to further augment the score based on what Part of Speech the word corresponds to. We boost the scores for noun/pronoun and reduce the scores for verb/preposition.Based on these mappings, we use Rocchio’s algorithm, with heuristically set values of alpha, beta, and gamma to obtain weights for each word which we haven’t already used in the query. We sort these in descending order of weights, and then take a heuristically determined number of top terms from this, marking their weights as negative infinity so that they are not chosen in future iterations.  Once we have these new terms, we create an index map for all query terms, remove all the non-query terms from the processed snippers of all relevant documents and then combine these by using the index map of query into one huge corpus. We use this digit corpus to find the most frequent permutation of different sizes for the query terms. We pick the permutation with highest frequency, based on which we pursue the next iterations of the query. The user is again prompted to label relevance and we continue these iterations till the desired precision is reached.F.Google Custom Search Engine JSON API Keep one of these on which we run our code.Swati: Key: AIzaSyDI07bUpnPo2QrQaNRza54wYpz3BlldbRYEngine ID: e6d037c2c6089967eJaidev: